{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22988,"status":"ok","timestamp":1691019421429,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"},"user_tz":-420},"id":"K1Fho190d00m","outputId":"18135e70-c54a-496b-ab58-f6f7afbd332f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["#@title Mount Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"n5zchAMDd4pH","executionInfo":{"status":"ok","timestamp":1691019480699,"user_tz":-420,"elapsed":34348,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"}}},"outputs":[],"source":["#@title Copy Models for Inference\n","!cp -r /content/gdrive/MyDrive/TugasAkhir/models models"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"952XnR99MDiP","executionInfo":{"status":"ok","timestamp":1691019540746,"user_tz":-420,"elapsed":60052,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"}}},"outputs":[],"source":["#@title Install Dependencies {display-mode: \"form\"}\n","from IPython.display import clear_output\n","\n","# YOLOv8 Dependencies\n","!pip install -q ultralytics\n","!pip install -q lap\n","\n","# BoT-SORT Dependencies\n","!pip install -q boxmot==10.0.16\n","\n","# OpenPose Dependencies\n","!git clone -q https://github.com/HaritsNasution/JAAD-TF-Pose-Estimation.git pose\n","!mkdir pose/models/graph/jaad && cp -r /content/models/OpenPose/jaad pose/models/graph\n","!pip install -q swig\n","!cd pose/ && pip install -q -r requirements.txt\n","!cd pose/tf_pose/pafprocess && swig -python -c++ pafprocess.i && python3 setup.py build_ext --inplace\n","!pip install -q git+https://github.com/adrianc-a/tf-slim.git@remove_contrib\n","\n","# I3D Dependencies\n","\n","clear_output(wait=False)\n","\n","exit()"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"oo7BFrWnn1Ei","executionInfo":{"status":"ok","timestamp":1691020062526,"user_tz":-420,"elapsed":10286,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"}}},"outputs":[],"source":["#@title Import Libraries {display-mode: \"form\"}\n","from IPython.display import clear_output\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import statistics as st\n","\n","import os\n","import time\n","import shutil\n","from google.colab import files\n","\n","# YOLO Lib\n","from ultralytics import YOLO\n","\n","# BoT-SORT Lib\n","from boxmot import BoTSORT\n","from pathlib import Path\n","\n","# Pose Lib\n","import sys\n","sys.path.append('/content/pose')\n","from tf_pose import common\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","\n","# I3D Lib\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"pL4tDlbfCflW"},"source":["# Load Model and Define Helper Functions"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-M6R6OO5vcrT","executionInfo":{"status":"ok","timestamp":1691020099522,"user_tz":-420,"elapsed":8320,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"}}},"outputs":[],"source":["# Load YOLO Model\n","yolo = YOLO('/content/models/YOLO/YOLOv8.3.pt')\n","\n","# Load Pose Model\n","model='jaad'\n","pose = TfPoseEstimator(get_graph_path(model), target_size=(224, 224))\n","\n","# Load CNN Model\n","cnn = torch.jit.load('/content/models/CNN/I3Dv4.pt')\n","cnn = cnn.cuda()\n","\n","clear_output(wait=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"f9WdlwSIdS9_","executionInfo":{"status":"ok","timestamp":1691020101106,"user_tz":-420,"elapsed":2,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"}}},"outputs":[],"source":["def get_human_pose(image, showBG = True):\n","\n","  humans = pose.inference(image, resize_to_default=True, upsample_size=4.0)\n","\n","  humans.sort(key=lambda human: human.score, reverse=True)\n","\n","  if showBG == False:\n","    image = np.zeros(image.shape)\n","\n","  image = pose.draw_humans(image, humans, imgcopy=False)\n","\n","  return image"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iCtkP95g6Ms4","executionInfo":{"status":"ok","timestamp":1691020102348,"user_tz":-420,"elapsed":2,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"}}},"outputs":[],"source":["def pred_func(seq, conf_thresh):\n","  # pred = model_j.predict(X_test[0:1], verbose=0)\n","  pred = torch.softmax(cnn(seq[0:1].cuda()), dim=1).detach().cpu().numpy()\n","  Y = pred[0][0]>=conf_thresh\n","\n","  return pred[0][0],Y"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48221,"status":"ok","timestamp":1691020157048,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"},"user_tz":-420},"id":"qDah9tg8T2Sl","outputId":"72c24a82-ce8b-49fd-e16a-5206943363c7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1Kkx2zW89jq_NETu4u42CFZTMVD5Hwm6e\n","To: /content/osnet_x0_25_msmt17.pt\n","100%|██████████| 9.34M/9.34M [00:00<00:00, 130MB/s]\n","\u001b[32m2023-08-02 23:49:09.778\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m229\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_msmt17.pt\"\u001b[0m\n","\u001b[32m2023-08-02 23:49:09.779\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m233\u001b[0m - \u001b[33m\u001b[1mThe following layers are discarded due to unmatched keys or layer size: ('classifier.weight', 'classifier.bias')\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Faiz3.mp4 finished in 0:1:48 seconds\n"]}],"source":["#@title Uji Coba\n","filename = \"Faiz3.mp4\"\n","\n","conf_thresh = 0.63\n","\n","# Hitung waktu runtime per video\n","start = time.time()\n","\n","# Pendefinisian input dan output\n","video_path = f\"/content/gdrive/MyDrive/TugasAkhir/DataSimulasi/{filename}\"\n","output_name = f\"/content/{os.path.splitext(filename)[0]}.avi\"\n","\n","# Detect with YOLOv8\n","results = yolo(video_path, stream=True, classes=0, verbose=False, conf=0.5, iou=0.5)\n","\n","# Initialize BoT-SORT Tracker\n","tracker = BoTSORT(\n","          model_weights=Path('osnet_x0_25_msmt17.pt'),  # which ReID model to use\n","          device='cuda:0',  # 'cpu', 'cuda:0', 'cuda:1', ... 'cuda:N'\n","          fp16=True,  # wether to run the ReID model with half precision or not\n","          )\n","\n","# Get video info, such as FPS and Size\n","cap = cv2.VideoCapture(video_path)\n","FPS = cap.get(cv2.CAP_PROP_FPS)\n","SIZE = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n","cap.release()\n","\n","# Define video writer to create output\n","out = cv2.VideoWriter(output_name, cv2.VideoWriter_fourcc(*'MJPG'), FPS, SIZE)\n","\n","# Initializations\n","rolling_data={}\n","cls = {}\n","count = -1\n","color = (255, 0, 0)\n","thickness = 2\n","patience = 5*30\n","cooldown = 0\n","K = 30\n","\n","# used to record the time when we processed last frame\n","prev_frame_time = 0\n","\n","# used to record the time at which we processed current frame\n","new_frame_time = 0\n","\n","# Process results generator\n","for result in results:\n","  # Read frame from YOLO\n","  frame = result.orig_img\n","  count+=1\n","\n","  # Get all bboxes in a frame\n","  boxes = result.boxes  # Boxes object for bbox outputs\n","  dets = boxes.data.cpu().numpy()\n","\n","  # Track detection results\n","  ts = tracker.update(dets,frame)\n","\n","  # Save ID that Appears\n","  ID = []\n","\n","  for d in ts:\n","    x0, y0 = tuple((np.array(d[0:2])).astype(np.int32))\n","    x0, y0 = np.max([(x0,y0),(0,0)],axis=0)\n","    x1, y1 = tuple((np.array(d[2:4])).astype(np.int32))\n","    x1, y1 = np.min([(x1,y1),(1920,1080)],axis=0)\n","    id = int(d[4])\n","\n","    y = 0\n","    conf = 0.5\n","\n","    if id in list(rolling_data.keys()):\n","      if len(rolling_data[id]) == 16:\n","        seq = ((torch.tensor(np.array(rolling_data[id])).permute(3,0,1,2)/255)-0.45)/0.225 # (3,16,224,224)\n","        seq = seq[None,:] # (1,3,16,224,224)\n","        conf,y = pred_func(seq,conf_thresh) # classification output\n","      else:\n","        seq = ((torch.tensor(np.array([rolling_data[id][-1]]*16)).permute(3,0,1,2)/255)-0.45)/0.225 # (3,16,224,224)\n","        seq = seq[None,:] # (1,3,16,224,224)\n","        conf,y = pred_func(seq,conf_thresh) # classification output\n","\n","    # Keep each ID's state\n","    if id in list(cls.keys()):\n","      if len(cls[id])<K:\n","        cls[id].append(y)\n","      else:\n","        del cls[id][0]\n","        cls[id].append(y)\n","    else:\n","      cls[id] = [y]\n","\n","    # Keep ID appearance in frame\n","    ID.append(id)\n","\n","    # Determine color and label of crossing action (C = Cross, NC = Not Cross)\n","    if y == 1:\n","      color = (0, 0, 255)\n","      lab = 'C'\n","    else:\n","      color = (0, 255, 0)\n","      lab = 'NC'\n","\n","    # Estimate pose for the detected pedestrian\n","    cropped = frame[y0:y1,x0:x1]\n","    frame[y0:y1,x0:x1] = get_human_pose(cropped)\n","\n","    # storing the data for last 16 frames\n","    try:\n","      if id in list(rolling_data.keys()): # ID exists in dict\n","        if len(rolling_data[id]) < 16: # bboxes values for 16 frames\n","          cropped_img = cv2.resize(frame[y0:y1, x0:x1],(224,224))\n","          rolling_data[id].append(np.asarray(cropped_img)) # append the image\n","        else:\n","          del rolling_data[id][0] # delete oldest frame bbox and append latest frame bbox\n","          cropped_img = cv2.resize(frame[y0:y1, x0:x1],(224,224))\n","          rolling_data[id].append(np.asarray(cropped_img))\n","      else:\n","        cropped_img = cv2.resize(frame[y0:y1, x0:x1],(224,224))\n","        rolling_data[id] = [np.asarray(cropped_img)]\n","    except:\n","      pass\n","\n","    # Annotate BBox\n","    frame = cv2.rectangle(frame, (x0,y0), (x1,y1), color, thickness)\n","\n","    # Allocate Text Size\n","    label = f\"ID:{id}|{lab}\"\n","    (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1, 1)\n","\n","    # Annotates ID and Label on BBox.\n","    frame = cv2.rectangle(frame, (x0-2, y0 - 30), (x0 + w, y0), color, -1)\n","    frame = cv2.putText(frame, label, (x0, y0 - 5), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n","\n","  new_frame_time = time.time()\n","\n","  # Calculates FPS and add to Video\n","  fps = 1/(new_frame_time-prev_frame_time)\n","  frame = cv2.putText(frame,f'FPS:{fps:.2f}',(0, 50),cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3)\n","\n","  # Calculate modes of state every K frames for car status (STOP or GO)\n","  if count%K == 0:\n","    check = [st.mode(cls[i]) for i in ID]\n","    if len(check)>0:\n","      if any(c!=0 for c in check):\n","        state = {'car':'STOP','color':(0,0,255)}\n","        cooldown = patience\n","      else:\n","        state = {'car':'GO','color':(0,255,0)} if cooldown==0 else state\n","    else:\n","      state = {'car':'GO','color':(0,255,0)} if cooldown==0 else state\n","\n","  # Annotating car status\n","  frame = cv2.putText(frame,f\"{state['car']}\",(0+5, 1080-10),cv2.FONT_HERSHEY_SIMPLEX, 3, state['color'], 5)\n","\n","  # Reduce STOP cooldown per frame\n","  cooldown = max(0,cooldown-1)\n","\n","  # Write frame into video\n","  out.write(frame)\n","\n","  prev_frame_time = new_frame_time\n","\n","out.release()\n","end = time.time() - start\n","print(f'{filename} finished in {end/3600:.0f}:{end/60%60:.0f}:{end%60:.0f} seconds')"]},{"cell_type":"code","source":["!cp Faiz3.avi /content/gdrive/MyDrive/Faiz3.avi"],"metadata":{"id":"FIdk1CwgqRUp","executionInfo":{"status":"ok","timestamp":1691020256746,"user_tz":-420,"elapsed":400,"user":{"displayName":"Harits Nasution","userId":"00163699128947912746"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SAUgzdrDqV4x"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1ESLNALn5vVPyXLndRCwiHrcOLKTuoE4T","timestamp":1691015817383}],"gpuType":"T4","collapsed_sections":["GZzbLMflbkbd","9ggisjtTbr_c"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}